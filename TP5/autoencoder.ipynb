{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import Autoencoder\n",
    "from parser import data_converter, print_letter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = np.array(data_converter(\"resources\"))\n",
    "# for char in alphabet:\n",
    "#     print_letter(char)\n",
    "# print(len(alphabet))\n",
    "flattened_input = np.array(list(map(lambda char: np.array(char).flatten(), alphabet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "errors =[]\n",
    "\n",
    "def callback_fun(e, error, w, o):\n",
    "    epochs.append(e)\n",
    "    errors.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "dot product:  [[1.19006462 1.27012853 1.20778451]\n",
      " [1.27012853 1.3555789  1.28904056]\n",
      " [1.20778451 1.28904056 1.22576824]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ana.cruz/Documents/ITBA/SIA/SIA-tps/TP5/multilayer_perceptron.py:123: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  m_t_hat = m_t/(1-beta_1**t)\n",
      "/Users/ana.cruz/Documents/ITBA/SIA/SIA-tps/TP5/multilayer_perceptron.py:124: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  v_t_hat = v_t/(1-beta_2**t)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ana.cruz/Documents/ITBA/SIA/SIA-tps/TP5/autoencoder.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ana.cruz/Documents/ITBA/SIA/SIA-tps/TP5/autoencoder.ipynb#ch0000003?line=1'>2</a>\u001b[0m activation_derivative \u001b[39m=\u001b[39m (\u001b[39mlambda\u001b[39;00m x: \u001b[39m2\u001b[39m\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m+\u001b[39mnp\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mx)))\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m+\u001b[39mnp\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mx)))) )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ana.cruz/Documents/ITBA/SIA/SIA-tps/TP5/autoencoder.ipynb#ch0000003?line=2'>3</a>\u001b[0m ae \u001b[39m=\u001b[39m Autoencoder(\u001b[39m35\u001b[39m, [], latent_dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, activation_function\u001b[39m=\u001b[39mactivation_function,activation_function_derivative\u001b[39m=\u001b[39mactivation_derivative, update_learn_rate\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, update_frequency\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(flattened_input),momentum\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, use_adam\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ana.cruz/Documents/ITBA/SIA/SIA-tps/TP5/autoencoder.ipynb#ch0000003?line=5'>6</a>\u001b[0m output, err_min \u001b[39m=\u001b[39m ae\u001b[39m.\u001b[39;49mtrain(flattened_input, expected_output\u001b[39m=\u001b[39;49mflattened_input, epoch_limit\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, callback\u001b[39m=\u001b[39;49mcallback_fun)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ana.cruz/Documents/ITBA/SIA/SIA-tps/TP5/autoencoder.ipynb#ch0000003?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(err_min)\n",
      "File \u001b[0;32m~/Documents/ITBA/SIA/SIA-tps/TP5/autoencoder.py:46\u001b[0m, in \u001b[0;36mAutoencoder.train\u001b[0;34m(self, training_set, expected_output, epoch_limit, callback)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m(expected_output \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     45\u001b[0m     expected_output\u001b[39m=\u001b[39mtraining_set\n\u001b[0;32m---> 46\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmultilayer_perceptron\u001b[39m.\u001b[39;49mtrain(training_set, expected_output, epoch_limit, callback)\n",
      "File \u001b[0;32m~/Documents/ITBA/SIA/SIA-tps/TP5/multilayer_perceptron.py:173\u001b[0m, in \u001b[0;36mMultiLayerPerceptron.train\u001b[0;34m(self, training_set, expected_output, epoch_limit, callback)\u001b[0m\n\u001b[1;32m    171\u001b[0m     dict1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mback_propagation(expected_output[idx])\n\u001b[1;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     dict1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madam(expected_output[idx], i)\n\u001b[1;32m    174\u001b[0m dict2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights_diff\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights_diff \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/ITBA/SIA/SIA-tps/TP5/multilayer_perceptron.py:130\u001b[0m, in \u001b[0;36mMultiLayerPerceptron.adam\u001b[0;34m(self, expected, t)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv[i] \u001b[39m=\u001b[39m v_t\n\u001b[1;32m    129\u001b[0m     \u001b[39m#wt - w(t-1)\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     weights_diff[i] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(alpha\u001b[39m/\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate)\u001b[39m*\u001b[39madam_alpha\u001b[39m/\u001b[39m(sqrt(v_t_hat)\u001b[39m+\u001b[39mepsilon)\n\u001b[1;32m    131\u001b[0m \u001b[39mreturn\u001b[39;00m weights_diff\n",
      "\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "\n",
    "activation_function = (lambda x: 1/(1+np.exp(-2*x)))\n",
    "activation_derivative = (lambda x: 2*(1/(1+np.exp(-2*x)))*(1-(1/(1+np.exp(-2*x)))) )\n",
    "ae = Autoencoder(35, [], latent_dim=2, activation_function=activation_function,activation_function_derivative=activation_derivative, update_learn_rate=None, learning_rate=0.001, update_frequency=len(flattened_input),momentum=False, use_adam=True)\n",
    "\n",
    "\n",
    "output, err_min = ae.train(flattened_input, expected_output=flattened_input, epoch_limit=1500, callback=callback_fun)\n",
    "\n",
    "print(err_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "print_letter(alphabet[i])\n",
    "reconstruct, reconstruction_error = ae.reconstruct(flattened_input[i])\n",
    "\n",
    "print_letter(reconstruct.reshape(alphabet[i].shape))\n",
    "print(reconstruction_error)\n",
    "\n",
    "latent_output = ae.encode(flattened_input[i])\n",
    "print(latent_output)\n",
    "print_letter(ae.decode(latent_output).reshape(alphabet[i].shape))\n",
    "\n",
    "ae.graph_latent_space(flattened_input)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
