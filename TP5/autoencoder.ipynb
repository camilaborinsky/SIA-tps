{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import Autoencoder\n",
    "from parser import data_converter, print_letter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from compare_architectures import sample_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set = list()\n",
    "training_set = None\n",
    "for i in [1]:\n",
    "        labels, alphabet = data_converter(\"resources/fonts_\" + str(i) + \".txt\")\n",
    "        alphabet = np.array(alphabet)\n",
    "        flattened_input = np.array(list(map(lambda char: np.array(char).flatten(), alphabet)))\n",
    "        if training_set is None:\n",
    "                training_set = flattened_input\n",
    "        else:\n",
    "                training_set = np.concatenate(training_set, sample_set(flattened_input, 0.5))\n",
    "        testing_set.append(list(zip(labels, flattened_input)))\n",
    "\n",
    "# for char in alphabet:\n",
    "#     print_letter(char)\n",
    "# print(len(alphabet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "errors =[]\n",
    "\n",
    "def callback_fun(e, error, w, o):\n",
    "    epochs.append(e)\n",
    "    errors.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "activation_function = (lambda x: 1/(1+np.exp(-2*x)))\n",
    "activation_derivative = (lambda x: 2*(1/(1+np.exp(-2*x)))*(1-(1/(1+np.exp(-2*x)))) )\n",
    "ae = Autoencoder(35, [], latent_dim=2, activation_function=activation_function,activation_function_derivative=activation_derivative, update_learn_rate=None, learning_rate=0.001, update_frequency=0,momentum=False, use_adam=False)\n",
    "\n",
    "\n",
    "output, err_min = ae.train(training_set, expected_output=training_set, epoch_limit=100, callback=callback_fun)\n",
    "\n",
    "test_error = ae.test_autoencoder(testing_set[0])\n",
    "print(test_error)\n",
    "plt.plot(epochs, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "errors =[]\n",
    "ae = Autoencoder(35, [], latent_dim=2, activation_function=activation_function,activation_function_derivative=activation_derivative, update_learn_rate=None, learning_rate=0.001, update_frequency=len(training_set),momentum=False, use_adam=False)\n",
    "\n",
    "\n",
    "output, err_min = ae.train(training_set, expected_output=training_set, epoch_limit=100, callback=callback_fun)\n",
    "\n",
    "test_error = ae.test_autoencoder(testing_set[0])\n",
    "print(test_error)\n",
    "plt.plot(epochs, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "errors =[]\n",
    "ae = Autoencoder(35, [], latent_dim=2, activation_function=activation_function,activation_function_derivative=activation_derivative, update_learn_rate=None, learning_rate=0.001, update_frequency=len(training_set),momentum=True, use_adam=False)\n",
    "\n",
    "\n",
    "output, err_min = ae.train(training_set, expected_output=training_set, epoch_limit=100, callback=callback_fun)\n",
    "\n",
    "test_error = ae.test_autoencoder(testing_set[0])\n",
    "print(test_error)\n",
    "plt.plot(epochs, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "errors =[]\n",
    "ae = Autoencoder(35, [], latent_dim=2, activation_function=activation_function,activation_function_derivative=activation_derivative, update_learn_rate=None, learning_rate=0.001, update_frequency=0,momentum=True, use_adam=False)\n",
    "\n",
    "\n",
    "output, err_min = ae.train(training_set, expected_output=training_set, epoch_limit=100, callback=callback_fun)\n",
    "\n",
    "test_error = ae.test_autoencoder(testing_set[0])\n",
    "print(test_error)\n",
    "plt.plot(epochs, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "errors =[]\n",
    "def adaptive_eta(eta, error_k):\n",
    "    consistent = 3 # La cantidad de epocas a partir de las cuales considero que el error crece o decrese _consistentemente_ \n",
    "    a = 0.01 # Constante de ajuste si el error crece\n",
    "    b = 0.03 #Ajuste si el error decrece \n",
    "    if error_k > consistent and error_k % consistent == 0: \n",
    "        #print(\"Error was increasing, adapt eta. Error_k: \", error_k) \n",
    "      return eta - b*eta \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif error_k < -consistent and error_k % consistent == 0:\n",
    "        #print(\"Error was decreasing, adapt eta, Error_k: \", error_k) \n",
    "        return eta + a\n",
    "\n",
    "    else:\n",
    "        return eta\n",
    "\n",
    "def no_adaptive_eta(eta, error_k):\n",
    "    return eta\n",
    "\n",
    "ae = Autoencoder(35, [25, 17], latent_dim=2, activation_function=activation_function,activation_function_derivative=activation_derivative, update_learn_rate=adaptive_eta, learning_rate=0.001, update_frequency=0,momentum=False, use_adam=False)\n",
    "\n",
    "\n",
    "output, err_min = ae.train(training_set, expected_output=training_set, epoch_limit=100, callback=callback_fun)\n",
    "\n",
    "test_error = ae.test_autoencoder(testing_set[0])\n",
    "print(test_error)\n",
    "plt.plot(epochs, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 10\n",
    "# print_letter(alphabet[i])\n",
    "# reconstruct, reconstruction_error = ae.reconstruct(flattened_input[i])\n",
    "\n",
    "# print_letter(reconstruct.reshape(alphabet[i].shape))\n",
    "# print(reconstruction_error)\n",
    "\n",
    "# latent_output = ae.encode(flattened_input[i])\n",
    "# print(latent_output)\n",
    "# print_letter(ae.decode(latent_output).reshape(alphabet[i].shape))\n",
    "\n",
    "# ae.graph_latent_space(flattened_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compare_architectures import compare_architectures\n",
    "#compare_architectures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parser import parse_config\n",
    "from compare_architectures import graph_evolution, graph_latent_space, graph_latent_space_by_font, graph_comp_error\n",
    "config_file = parse_config(\"resources/config.json\")\n",
    "input_values = list(map(lambda a: str(a), config_file[\"architectures\"]))\n",
    "graph_evolution(0, input_values, comparing_attribute=\"architecture\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_latent_space(0, input_values, comparing_attribute=\"architecture\", which_font=-1, show_labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_latent_space(0, [\"[25, 17, 8, 4]\", \"[17, 8, 4]\" ], comparing_attribute=\"special_architecture\", which_font=-1, show_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same architecture, divide by font:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_latent_space_by_font(0, [\"[25, 17]\"], comparing_attribute=\"special_architecture\", show_labels=True, which_fonts=[0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two best: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_latent_space(0, [\"[25, 17]\", \"[25, 17, 8]\" ], comparing_attribute=\"special_architecture\", which_font=0, show_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two with worse error: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_latent_space(0, [\"[25, 17, 8, 4]\", \"[17, 8, 4]\" ], comparing_attribute=\"special_architecture\", which_font=0, show_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_comp_error(input_values, comparing_attribute=\"architecture\", execution_count=2, separate_by_font=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5  # figure with 15x15 digits\n",
    "digit_size = 80\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian\n",
    "# to produce values of the latent variables z, since the prior of the latent space is Gaussian\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "print(grid_y)\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
