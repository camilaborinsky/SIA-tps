{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.file_utils as fu\n",
    "import perceptrons.multilayer_perceptron as mlp\n",
    "import numpy as np\n",
    "import metrics as m\n",
    "import graphing as g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = \"ex_2/resources/config.json\"\n",
    "config_file = fu.parse_config(config_file_path)\n",
    "\n",
    "path_to_data = config_file[\"path_to_data\"]\n",
    "epoch_limit = int(config_file[\"epoch_limit\"])\n",
    "execution_count = int(config_file[\"execution_count\"])\n",
    "momentum = bool(config_file[\"momentum\"])\n",
    "cross_validation_k = int(config_file[\"cross_validation\"])\n",
    "learn_rate = float(config_file[\"learn_rate\"])\n",
    "adaptive_learn_rate = bool(config_file[\"adaptive_learn_rate\"])\n",
    "if adaptive_learn_rate:\n",
    "    update_learn_rate = (lambda lr, k: lr + 0.3 if k >= 3 else (lr - lr*0.1 if k <=-3 else 0)) \n",
    "else:\n",
    "    update_learn_rate = None\n",
    "if config_file[\"activation_function\"] == \"linear\":\n",
    "    activation_function = (lambda x: x)\n",
    "    activation_derivative = (lambda x: 1)\n",
    "elif config_file[\"activation_function\"] == \"sigmoid\":\n",
    "    activation_function = (lambda x: 1/(1+np.exp(-2*x)))\n",
    "    activation_derivative = (lambda x: 2*(1/(1+np.exp(-2*x)))*(1-(1/(1+np.exp(-2*x)))) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, expected = fu.parse_training_set_from_file(path_to_data)\n",
    "p = mlp.MultiLayerPerceptron(learning_rate=learn_rate, hidden_layers=[], input_dim=len(training[0]), output_dim=len(expected[0]), update_frequency=0, activation_function=activation_function, activation_function_derivative=activation_derivative, update_learn_rate=update_learn_rate, scale_output=True,momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "open(path_to_data+\"/error_vs_iteration.txt\", \"w\").close()\n",
    "p.train(training, expected, epoch_limit, callback=(lambda i, error, weights, output: fu.write_error_vs_iteration(path_to_data, error, i) if i%len(training) == 0 else None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.error_vs_iteration(path_to_data, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i in range(execution_count):\n",
    "    w, e = p.train(training, expected, epoch_limit, callback=None)\n",
    "    errors.append(e)\n",
    "f = open(path_to_data+\"/learning_rates.txt\", \"a\")\n",
    "f.write(\"{},{},{}\\n\".format(learn_rate, np.mean(errors), np.std(errors)))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_file[\"activation_function\"] == \"sigmoid\":\n",
    "  \n",
    "    training_groups, expected_groups = m.cross_validation(training,cross_validation_k,expected)\n",
    "    err_mins = []\n",
    "    rmsd_mins = []\n",
    "    for i in range(execution_count):\n",
    "        err_min = None\n",
    "        rmsd_min = None\n",
    "        for i, test_group in enumerate(training_groups):\n",
    "            errors =[]\n",
    "            t = training_groups[:i] + training_groups[i+1:]\n",
    "            training_set = [item for sublist in t for item in sublist]\n",
    "            e = expected_groups[:i] + expected_groups[i+1:]\n",
    "            expected_set = [item for sublist in e for item in sublist]\n",
    "            print(len(training_set))\n",
    "            print(len(training_set[0]))\n",
    "            w, err = p.train(training_set, expected_set, epoch_limit, callback=None)\n",
    "        \n",
    "            # Test\n",
    "            for j, test_entry in enumerate(test_group):\n",
    "                prediction = p.forward_propagation(test_entry)\n",
    "                expected_output = expected_groups[i][j]\n",
    "                errors.append(np.abs(np.subtract(prediction , expected_output)))\n",
    "\n",
    "            # mean_error = m.mean_error(errors)\n",
    "            max_error = m.max_error(errors)\n",
    "            # min_error = m.min_error(errors)\n",
    "            # mse_error = m.mse(errors)\n",
    "            rmsd_error = m.rmsd(errors)\n",
    "            if err_min == None or err < err_min:\n",
    "                err_min = err\n",
    "            if rmsd_min == None or rmsd_error < rmsd_min:\n",
    "                rmsd_min = rmsd_error\n",
    "        err_mins.append(err_min)\n",
    "        rmsd_mins.append(rmsd_min)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path_to_data+\"/k_vs_errors.txt\", \"a\")\n",
    "f.write(\"{},{},{},{},{}\\n\".format(cross_validation_k, np.mean(err_mins), np.std(err_mins), np.mean(rmsd_mins), np.std(rmsd_mins)))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
